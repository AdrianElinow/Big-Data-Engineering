{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae81d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = os.environ['SPARK_HOME']\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.9-src.zip\")\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = 4\n",
    "memory_gb = 8\n",
    "\n",
    "conf = (pyspark.SparkConf().setMaster('local[{}]'.format(number_cores)).set('spark.driver.memory', '{}g'.format(memory_gb)))\n",
    "\n",
    "conf\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "775db154",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sc.textFile(\"./sm_graph.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43c2a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 adrian  staff  19 Sep 26 10:45 ./sm_graph.dat\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./sm_graph.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a52366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y y', 'y a', 'a y', 'a m', 'm a']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71d541",
   "metadata": {},
   "source": [
    "Matrix formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd92fcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', ['y', 'a']), ('a', ['y', 'm']), ('m', ['a'])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = graph.map( lambda line: (line.split(\" \")[0], line.split(' ')[1])).groupByKey().mapValues(list)\n",
    "links.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2966f0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', 0.3333333333333333),\n",
       " ('a', 0.3333333333333333),\n",
       " ('m', 0.3333333333333333)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = links.count()\n",
    "ranks = links.map(lambda line: (line[0], 1/N) )\n",
    "ranks.take(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b08e879f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', (0.3333333333333333, ['y', 'a'])),\n",
       " ('a', (0.3333333333333333, ['y', 'm'])),\n",
       " ('m', (0.3333333333333333, ['a']))]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes = ranks.join(links)\n",
    "votes.take(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "04080d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', 0.16666666666666666), ('a', 0.16666666666666666)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateVotes(t):\n",
    "    res = []\n",
    "    for item in t[1][1]:\n",
    "        count = len(t[1][1])\n",
    "        res.append((item, t[1][0] / count))\n",
    "    return res\n",
    "\n",
    "calculate_votes( ('y',(0.3333333333333333, ['y','a'])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82aaacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', (0.3333333333333333, ['y', 'a'])),\n",
       " ('a', (0.3333333333333333, ['y', 'm'])),\n",
       " ('m', (0.3333333333333333, ['a']))]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f83c6f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 242.0 failed 1 times, most recent failure: Lost task 1.0 in stage 242.0 (TID 321, 104.201.133.117, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-5df8af9f3253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_votes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 242.0 failed 1 times, most recent failure: Lost task 1.0 in stage 242.0 (TID 321, 104.201.133.117, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "votes = ranks.flatMap(calculate_votes)\n",
    "votes.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10c1fce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 161.0 failed 1 times, most recent failure: Lost task 1.0 in stage 161.0 (TID 230, 104.201.133.117, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a097bf9069d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 161.0 failed 1 times, most recent failure: Lost task 1.0 in stage 161.0 (TID 230, 104.201.133.117, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor70.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/users/adrian/spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-17-71f62ddc8765>\", line 3, in calculate_votes\nTypeError: 'float' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "ranks = votes.reduceByKey(lambda x, y: x + y)\n",
    "ranks.take(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a48d515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.values().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "befb82c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'flatmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'flatmap'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    votes = ranks.flatmap(calculate_votes)\n",
    "    ranks = votes.reduceByKey(lambda x, y: x + y)\n",
    "    print(ranks.take(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d085b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'flatmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-e53dac42a1a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0msum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mold_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_votes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_ranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'flatmap'"
     ]
    }
   ],
   "source": [
    "sum = 1 \n",
    "while sum > 0.1:\n",
    "    old_ranks = ranks\n",
    "    votes = ranks.flatmap(calculate_votes)\n",
    "    ranks = votes.reduceByKey(lambda x, y: x + y)\n",
    "    errors = old_ranks.join(ranks).mapValues(lambda v: abs(v[0] - v[1]))\n",
    "    sum = errors.values().sum()\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b41f13",
   "metadata": {},
   "source": [
    "Hollins Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82702595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29888 ./hollins.dat\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./hollins.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67d22c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile('./hollins.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6d2a6f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6012 23875',\n",
       " '1 http://www1.hollins.edu/ ',\n",
       " '2 http://www.hollins.edu/ ',\n",
       " '3 http://www1.hollins.edu/Docs/CompTech/Network/webmail_faq.htm ',\n",
       " '4 http://www1.hollins.edu/Docs/Forms/GetForms.htm ',\n",
       " '5 http://www1.hollins.edu/Docs/misc/travel.htm ',\n",
       " '6 http://www1.hollins.edu/Docs/GVCalendar/gvmain.htm ',\n",
       " '7 http://www1.hollins.edu/docs/events/events.htm ',\n",
       " '8 http://www1.hollins.edu/docs/comptech/mainviruses.htm ',\n",
       " '9 http://www1.hollins.edu/Docs/Academics/acad.htm ',\n",
       " '10 http://www1.hollins.edu/Docs/CompTech/Blackboard/bb_faq.htm ',\n",
       " '11 http://www1.hollins.edu/Docs/comptech/comptech.htm ',\n",
       " '12 http://www1.hollins.edu/Docs/Academics/international_programs/index.htm ',\n",
       " '13 http://www1.hollins.edu/Docs/academics/online/cyber.htm ',\n",
       " '14 http://www1.hollins.edu/Registrar/registrar.htm ',\n",
       " '15 http://www1.hollins.edu/Docs/Academics/writingcenter/wcenter.htm ',\n",
       " '16 http://www.hollins.edu/about/map/map.htm ',\n",
       " '17 http://www1.hollins.edu/Docs/SchoolServices/FoodServ/default.htm ',\n",
       " '18 http://www1.hollins.edu/Docs/CampusLife/StudentAct/studentorgs/SGA/default.html ',\n",
       " '19 http://www1.hollins.edu/docs/admin/admin.htm ',\n",
       " '20 http://www1.hollins.edu/Docs/AlumDev/Default.htm ',\n",
       " '21 http://www1.hollins.edu/docs/athletics/athletics.htm ',\n",
       " '22 http://www1.hollins.edu/Docs/CampusLife/CampusLife.htm ',\n",
       " '23 http://www1.hollins.edu/Docs/Intercultural/default.htm ',\n",
       " '24 http://www1.hollins.edu/Docs/SchoolServices/SchoolServ.htm ',\n",
       " '25 http://www1.hollins.edu/security/Default.htm ',\n",
       " '26 http://www.hollins.edu/students/index.htm ',\n",
       " '27 http://www.hollins.edu/admissions/admissions.htm ',\n",
       " '28 http://www.hollins.edu/academics/academics.htm ',\n",
       " '29 http://www.hollins.edu/grad/coedgrad.htm ',\n",
       " '30 http://www.hollins.edu/academics/internships/real_world.htm ',\n",
       " '31 http://www.hollins.edu/campuslife/campus_life.htm ',\n",
       " '32 http://www.hollins.edu/abroad/abroad.htm ',\n",
       " '33 http://www.hollins.edu/careers/developctr/career_development.htm ',\n",
       " '34 http://www.hollins.edu/athletics/athletics.htm ',\n",
       " '35 http://www.hollins.edu/specprog/hollinsummer/holsum.htm ',\n",
       " '36 http://www.hollins.edu/news-events/news_events.htm ',\n",
       " '37 http://www.hollins.edu/admissions/visit/visit.htm ',\n",
       " '38 http://www.hollins.edu/about/about_tour.htm ',\n",
       " '39 http://www.hollins.edu/prospectives/index.htm ',\n",
       " '40 http://www.hollins.edu/alumnae/alumnae.htm ',\n",
       " '41 http://www.hollins.edu/parents/index.htm ',\n",
       " '42 http://www.hollins.edu/community/index.htm ',\n",
       " '43 http://www.hollins.edu/admissions/apply/apply.htm ',\n",
       " '44 http://www.hollins.edu/undergrad/undergraduate.htm ',\n",
       " '45 http://www.hollins.edu/undergrad/english/engcwrit.htm ',\n",
       " '46 http://www.hollins.edu/cgi-bin/htsearch ',\n",
       " '47 http://www.hollins.edu/sitemap/sitemap.htm ',\n",
       " '48 http://www.hollins.edu/contact/contact.htm ',\n",
       " '49 http://www.hollins.edu/tour/welcome.html ']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "889aa640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29888\n"
     ]
    }
   ],
   "source": [
    "lc = raw_data.count()\n",
    "print(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "138dfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29887"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_vals = [ int(x) for x in raw_data.first().split() ]\n",
    "header_vals[0] + header_vals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "58740c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5378 5653',\n",
       " '624 27',\n",
       " '2081 880',\n",
       " '649 655',\n",
       " '3587 http://www1.hollins.edu/classes/dance/website/vanessaweb/resume.html ',\n",
       " '1208 1210',\n",
       " '297 252',\n",
       " '5184 3950',\n",
       " '45 294',\n",
       " '775 176',\n",
       " '1877 822',\n",
       " '1544 66',\n",
       " '3236 3252',\n",
       " '161 157',\n",
       " '5924 4806',\n",
       " '5380 5711',\n",
       " '3941 3974',\n",
       " '798 http://www.hollins.edu/athletics/volleyball/schedule.htm ',\n",
       " '5504 5884',\n",
       " '1188 446',\n",
       " '2127 3157',\n",
       " '4736 http://www1.hollins.edu/registrar/MAJORS-MINORS/MAJ-MIN%2002-03/HISTORY%20MINOR.pdf ',\n",
       " '2536 3326',\n",
       " '2829 http://www1.hollins.edu/docs/gvcalendar/gvarchive/gvjuly23.htm ',\n",
       " '1027 http://www1.hollins.edu/ssforms/Advising.asp ',\n",
       " '1019 http://www.hollins.edu/undergrad/womenstudies/womfac.htm ',\n",
       " '5401 5397',\n",
       " '74 38',\n",
       " '3601 http://www1.hollins.edu/classes/dance/website/nicole/1.html ',\n",
       " '222 890',\n",
       " '880 886',\n",
       " '4024 4023',\n",
       " '818 37',\n",
       " '1410 2460',\n",
       " '522 http://www.hollins.edu/admissions/ugradadm/international/map2.htm ',\n",
       " '1208 69',\n",
       " '21 92',\n",
       " '3884 http://www1.hollins.edu/faculty/saloweyca/clas%20395/DAGEO/sld010.htm ',\n",
       " '66 427',\n",
       " '3879 http://www1.hollins.edu/faculty/saloweyca/clas%20395/DAGEO/sld005.htm ',\n",
       " '5614 http://www1.hollins.edu/homepages/saloweyca/ancpaint_files/slide0019.htm ',\n",
       " '2600 http://www1.hollins.edu/classes/comm232/sports.jeffreys.htm ',\n",
       " '3902 3905',\n",
       " '181 34',\n",
       " '412 55',\n",
       " '627 21',\n",
       " '3578 4734',\n",
       " '5399 5400',\n",
       " '1514 1509',\n",
       " '821 2']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.takeSample(False, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6a7bf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(n):\n",
    "    try:\n",
    "        tmp = int(n)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "59f0e98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23876"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_num = raw_data.filter(lambda line: is_numeric(line.split(\" \")[1]))\n",
    "num_num.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f44f0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6012"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_text = raw_data.filter(lambda line: not is_numeric(line.split(\" \")[1]))\n",
    "num_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9494669a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  ['2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '17',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '23',\n",
       "   '24',\n",
       "   '25']),\n",
       " ('8',\n",
       "  ['2',\n",
       "   '7',\n",
       "   '19',\n",
       "   '21',\n",
       "   '85',\n",
       "   '87',\n",
       "   '88',\n",
       "   '90',\n",
       "   '91',\n",
       "   '92',\n",
       "   '221',\n",
       "   '222',\n",
       "   '223',\n",
       "   '224',\n",
       "   '225',\n",
       "   '226',\n",
       "   '227',\n",
       "   '228',\n",
       "   '229',\n",
       "   '230',\n",
       "   '340',\n",
       "   '341',\n",
       "   '342',\n",
       "   '343']),\n",
       " ('16',\n",
       "  ['2',\n",
       "   '27',\n",
       "   '28',\n",
       "   '37',\n",
       "   '38',\n",
       "   '42',\n",
       "   '48',\n",
       "   '52',\n",
       "   '61',\n",
       "   '73',\n",
       "   '74',\n",
       "   '75',\n",
       "   '76',\n",
       "   '77',\n",
       "   '78',\n",
       "   '79']),\n",
       " ('20',\n",
       "  ['2',\n",
       "   '217',\n",
       "   '248',\n",
       "   '482',\n",
       "   '483',\n",
       "   '484',\n",
       "   '485',\n",
       "   '486',\n",
       "   '487',\n",
       "   '488',\n",
       "   '489',\n",
       "   '490',\n",
       "   '491',\n",
       "   '492',\n",
       "   '493']),\n",
       " ('26',\n",
       "  ['2', '27', '28', '37', '38', '39', '40', '41', '42', '43', '52', '61']),\n",
       " ('29',\n",
       "  ['2',\n",
       "   '27',\n",
       "   '28',\n",
       "   '37',\n",
       "   '38',\n",
       "   '43',\n",
       "   '52',\n",
       "   '61',\n",
       "   '73',\n",
       "   '118',\n",
       "   '119',\n",
       "   '120',\n",
       "   '121',\n",
       "   '122',\n",
       "   '123',\n",
       "   '124',\n",
       "   '125',\n",
       "   '126',\n",
       "   '127',\n",
       "   '128',\n",
       "   '129',\n",
       "   '130',\n",
       "   '131']),\n",
       " ('33',\n",
       "  ['2',\n",
       "   '27',\n",
       "   '28',\n",
       "   '30',\n",
       "   '37',\n",
       "   '38',\n",
       "   '40',\n",
       "   '43',\n",
       "   '52',\n",
       "   '61',\n",
       "   '73',\n",
       "   '168',\n",
       "   '169',\n",
       "   '170',\n",
       "   '171',\n",
       "   '172',\n",
       "   '173']),\n",
       " ('34',\n",
       "  ['2',\n",
       "   '37',\n",
       "   '38',\n",
       "   '43',\n",
       "   '52',\n",
       "   '73',\n",
       "   '84',\n",
       "   '148',\n",
       "   '174',\n",
       "   '175',\n",
       "   '176',\n",
       "   '177',\n",
       "   '178',\n",
       "   '179',\n",
       "   '180',\n",
       "   '181',\n",
       "   '182',\n",
       "   '183',\n",
       "   '184',\n",
       "   '185',\n",
       "   '186',\n",
       "   '187',\n",
       "   '188',\n",
       "   '189']),\n",
       " ('44',\n",
       "  ['2',\n",
       "   '28',\n",
       "   '34',\n",
       "   '37',\n",
       "   '38',\n",
       "   '43',\n",
       "   '45',\n",
       "   '52',\n",
       "   '61',\n",
       "   '132',\n",
       "   '261',\n",
       "   '262',\n",
       "   '263',\n",
       "   '264',\n",
       "   '265',\n",
       "   '266',\n",
       "   '267',\n",
       "   '268',\n",
       "   '269',\n",
       "   '270',\n",
       "   '271',\n",
       "   '272',\n",
       "   '273',\n",
       "   '274',\n",
       "   '275',\n",
       "   '276',\n",
       "   '277',\n",
       "   '278',\n",
       "   '279',\n",
       "   '280',\n",
       "   '281',\n",
       "   '282',\n",
       "   '283',\n",
       "   '284',\n",
       "   '285',\n",
       "   '286',\n",
       "   '287',\n",
       "   '288',\n",
       "   '289',\n",
       "   '290',\n",
       "   '291',\n",
       "   '292',\n",
       "   '293']),\n",
       " ('45',\n",
       "  ['2',\n",
       "   '28',\n",
       "   '37',\n",
       "   '38',\n",
       "   '43',\n",
       "   '44',\n",
       "   '52',\n",
       "   '61',\n",
       "   '123',\n",
       "   '132',\n",
       "   '198',\n",
       "   '252',\n",
       "   '294',\n",
       "   '295',\n",
       "   '296',\n",
       "   '297',\n",
       "   '298',\n",
       "   '299',\n",
       "   '300',\n",
       "   '301',\n",
       "   '302'])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = num_num.map(lambda line: (line.split(\" \")[0], line.split(\" \")[1])) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(list)\n",
    "links.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a37311a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 0.0003541076487252125),\n",
       " ('8', 0.0003541076487252125),\n",
       " ('16', 0.0003541076487252125),\n",
       " ('20', 0.0003541076487252125),\n",
       " ('26', 0.0003541076487252125),\n",
       " ('29', 0.0003541076487252125),\n",
       " ('33', 0.0003541076487252125),\n",
       " ('34', 0.0003541076487252125),\n",
       " ('44', 0.0003541076487252125),\n",
       " ('45', 0.0003541076487252125)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = links.count()\n",
    "ranks = links.map(lambda line: (line[0], 1 / N))\n",
    "ranks.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "773bdd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', 0.16666666666666666), ('a', 0.16666666666666666)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateVotes(t):\n",
    "    res = []\n",
    "    for item in t[1][1]:\n",
    "        count = len(t[1][1])\n",
    "        res.append((item, t[1][0] / count))\n",
    "    return res\n",
    "calculateVotes(('y', (0.3333333333333333, ['y', 'a'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d59263dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0289126892749134\n",
      "0.6524166912065938\n",
      "0.354732060058874\n",
      "0.20752688756812687\n",
      "0.14669618513501703\n",
      "0.11380517976401004\n",
      "0.09642834963258069\n",
      "CPU times: user 312 ms, sys: 60.7 ms, total: 373 ms\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum = 1\n",
    "N = links.count()\n",
    "ranks = links.map(lambda line: (line[0], 1 / N))\n",
    "ranks.take(10)\n",
    "\n",
    "while sum > 0.1:\n",
    "    old_ranks = ranks\n",
    "    votes = ranks.join(links) \\\n",
    "        .flatMap(calculateVotes)\n",
    "    ranks = votes.reduceByKey(lambda x, y: x + y)\n",
    "    errors = old_ranks.join(ranks).mapValues(lambda v: abs(v[0] - v[1]))\n",
    "    sum = errors.values().sum()\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb49f66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 0.01480246014081635),\n",
       " ('37', 0.010738970689076617),\n",
       " ('38', 0.009867983441807206),\n",
       " ('61', 0.009734731227889629),\n",
       " ('52', 0.009357107790050318),\n",
       " ('425', 0.00868217558595644),\n",
       " ('43', 0.008005883519059315),\n",
       " ('27', 0.00761747437182122),\n",
       " ('28', 0.006066068156415338),\n",
       " ('29', 0.0056368097448235835)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.takeOrdered(10,key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c0ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
